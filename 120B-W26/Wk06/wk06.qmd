---
title: "PSTAT 120B: Mathematical Statistics"
subtitle: "Ethan's Week 6 Discussion Section"
footer: "PSTAT 120B - Winter 2026 with Dr. Annie Qu; material © Ethan P. Marzban"
logo: "Images/120b_logo.svg"
format: 
  clean-revealjs:
    theme: ../slides.scss
    transition: fade
    slide-number: true
    incremental: true 
    chalkboard: true
    menu:
      side: left
html-math-method:
  method: mathjax
  url: "https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js"
author:
  - name: Ethan P. Marzban
    affiliations: Department of Statistics and Applied Probability; UCSB <br /> <br />
institute: February 11, 2026
title-slide-attributes:
    data-background-image: "Images/120b_logo.svg"
    data-background-size: "30%"
    data-background-opacity: "0.5"
    data-background-position: 80% 50%
code-annotations: hover
---

<style>
mjx-math {
  font-size: 80% !important;
}
</style>

<script>
MathJax = {
  options: {
    menuOptions: {
      settings: {
        assistiveMml: false
      }
    }
  }
};
</script>
<script type="text/javascript" id="MathJax-script" async src="path-to-MathJax/tex-chtml.js"></script>


$$
\newcommand\R{\mathbb{R}}
\newcommand{\N}{\mathbb{N}}
\newcommand{\E}{\mathbb{E}}
\newcommand{\S}{\mathbb{S}}
\newcommand{\Prob}{\mathbb{P}}
\newcommand{\F}{\mathcal{F}}
\newcommand{\1}{1\!\!1}
\newcommand{\comp}[1]{#1^{\complement}}
\newcommand{\Var}{\mathrm{Var}}
\newcommand{\SD}{\mathrm{SD}}
\newcommand{\vect}[1]{\vec{\boldsymbol{#1}}}
\newcommand{\tvect}[1]{\vec{\boldsymbol{#1}}^{\mathsf{T}}}
\newcommand{\hvect}[1]{\widehat{\boldsymbol{#1}}}
\newcommand{\mat}[1]{\mathbf{#1}}
\newcommand{\tmat}[1]{\mathbf{#1}^{\mathsf{T}}}
\newcommand{\Cov}{\mathrm{Cov}}
\DeclareMathOperator*{\argmin}{\mathrm{arg} \ \min}
\newcommand{\iid}{\stackrel{\mathrm{i.i.d.}}{\sim}}
\newcommand{\probto}{\stackrel{\mathrm{p}}{\longrightarrow}}
\newcommand{\distto}{\stackrel{\mathrm{d}}{\longrightarrow}}
\newcommand{\mseto}{\stackrel{\mathrm{m.s.}}{\longrightarrow}}
$$

```{css echo = F}
.hscroll {
  height: 100%;
  max-height: 600px;
  max-width: 2000px;
  overflow: scroll;
}
```

```{r setup, echo = F}
library(tidyverse)
library(countdown)
library(fixest)
library(modelsummary) # Make sure you have >=v2.0.0
library(GGally)
library(ggokabeito)
library(reshape2)
library(pander)
library(gridExtra)
library(cowplot)
library(palmerpenguins)
library(plotly)
library(tidymodels)
```

## {{< fa arrow-right >}} Convergence
### General Overview

-   Much like how certain sequences of _numbers_ converge in various ways, so too do certain sequences of _random variables_.

-   The three types of convergence we discussed in this class are:
    -   Convergence in Probability [P]
    -   Convergence in Distribution [D]
    -   Convergence in Mean Square [MS]
    
-   [MS] implies [P], but not necessarily vice-versa
-   [P] implies [D], but not necessarily vice-versa
    -   Question: Does [MS] imply [D]?
    
## {{< fa arrow-right >}} Convergence
### Convergence in Probability

::: {.callout-note}
## **Definition:** Convergence in Probability

We say that a sequence of random variables $\{X_n\}$ [**converges in probability**]{.alert} to a constant $x$ if, for any $\varepsilon > 0$,
$$ \lim_{n \to \infty} \Prob(|X_n - x| > \varepsilon) = 0$$
We use the notation $X_n \probto x$ to denote convergence in probability.
:::

-   $|X_n - x|$ is essentially the "distance" between $X_n$ and $x$.
    -   Therefore, the event $\{|X_n - x| > \varepsilon\}$ is essentially "$X_n$ is very far from $x$"
    -   Convergence in probability asserts that the probability of this event goes to zero as $n$ gets large; i.e. "we become more more and more certain that $X_n$ is very close to $x$"
    
    
    
## {{< fa arrow-right >}} Convergence
### Convergence in Probability: Example

```{r}
#| echo: True
#| code-fold: True

set.seed(100)
X <- sample(c("H", "T"), size = 1000, replace = T, prob = c(0.25, 0.75))
rel.freq <- cumsum(ifelse(X == "H", 1, 0)[1:1000])/(1:1000)
```

```{r}
data.frame(x = 1:1000, y = rel.freq) %>%
  plot_ly(
    x = ~x,
    y = ~y,
    type = "scatter",
    mode = "lines",
    text = ~paste("<br>Num. Tosses:", x,
                  "<br>Prop. Heads:", round(y, 6))
  ) %>%
  layout(title = 'Relative Proportion of Heads')
```


## {{< fa maximize >}} Law of Large Numbers
### Two Versions

::: {.callout-important}
## **Law of Large Numbers** (Version 2)

If $X_1, X_2, \cdots, X_n$ denotes an i.i.d. sample from a distribution with finite mean $\E[X_i]$ and finite variance $\Var(X_i)$, then
$$ \bar{X}_n \probto \E[X_i] $$
:::

::: {.fragment}
::: {.callout-important}
## **Law of Large Numbers** (Version 1)

If $X_1, X_2, \cdots, X_n$ denotes an i.i.d. sample from a distribution with finite mean $\E[X_i]$ and finite variance $\Var(X_i)$, then
$$ \bar{X}_n \mseto \E[X_i] $$
:::
:::

::: {.fragment}
$$ f_X(x) = \frac{1}{\pi(1 + x^2)} $$
:::


## {{< fa maximize >}} Law of Large Numbers
### Infinite Mean and Variance Example

```{r}
#| echo: True
#| code-fold: True

set.seed(100)
X <- rcauchy(1000)
rel.freq <- cumsum(X)/(1:1000)
```


```{r}
data.frame(x = 1:1000, y = rel.freq) %>%
  plot_ly(
    x = ~x,
    y = ~y,
    type = "scatter",
    mode = "lines",
    text = ~paste("<br>Trial Num.:", x,
                  "<br>Cumulative Avg.:", round(y, 6))
  ) %>%
  layout(title = 'Cumulative Average')
```



## {{< fa dice >}} Probability vs. Statistics
### An Illustration

-   Consider a bucket comprised of blue and gold marbles, and suppose we know how many blue and gold marbles there are.
    -   From this bucket, we take a sample of marbles.
    
:::: {.columns}

::: {.column width="40%"}
::: {.fragment}
![](images/prob_vs_stat_1.svg)
:::
:::

::: {.column width="60%"}
-   We then use our information of the configuration of marbles in the bucket to inform us about what's in our hand.
    -   E.g. what's the expected number of gold marbles in our hand?
    -   E.g. what's the probability that we have more than 3 blue marbles in our hand?
:::

::::



## {{< fa dice >}} Probability vs. Statistics
### An Illustration

-   Consider now the opposite scenario: we do _not_ know anything about the configuration of marbles in the bucket. All we have is a sample of, say, 11 blue and 6 gold marbles drawn from this bucket.
    
:::: {.columns}

::: {.column width="40%"}
::: {.fragment}
![](images/prob_vs_stat_2.svg)
:::
:::

::: {.column width="60%"}
-   We then use our information on the configuration of marbles in our _hand_ to inform us about what's in the _bucket_.
    -   E.g. what's the expected number of gold marbles in the bucket?
    -   E.g. what's the probability that there are more than 3 blue marbles in the bucket?
:::

::::




## {{< fa magnifying-glass-chart >}} Statistical Inference
### General Framework

-   Now, instead of marbles in a bucket, imagine _units_ in our _sampled population_.


:::: {.columns}

::: {.column width="60%"}
-   We have a [**population**]{.alert}, governed by a set of [**population parameters**]{.alert} that are unobserved (but that we’d like to make claims about).

-   To make claims about the population parameters, we take a [**sample**]{.alert}.

-   We then use our sample to make [**inferences**]{.alert} (i.e. claims) about the population parameters.


:::

::: {.column width="40%"}
::: {.fragment}
![](images/inference.svg)
:::
:::

::::


## {{< fa sign-hanging >}} Estimation
### General Framework

-   Three key terms:
    -   [**Estimand:**]{.alert} another word for the parameter we are trying to estimate. (**Not in lecture slides!**)
    -   [**Estimator:**]{.alert} a statistic being used to estimate the estimand.
        -   Another way to think about this: a "rule" used to estimate the parameter.
    -   [**Estimate:**]{.alert} a particular _realization_ (i.e. _observed instance_) of an estimator.
    
## {{< fa sign-hanging >}} Estimation
### Example

::: {.callout-tip}
## **Example**

A vet wishes to estimate the true weight of all cats in the world. She takes a sample of 10 cats, and finds their average weight to be 9.12 lbs.
:::

-   The **estimand** is the true average weight of all cats in the world (which we can call µ).

-   The **estimator** is the sample mean: we are using sample means to estimate µ.

-   The **estimate** in this scenario is 9.12 lbs, as this is a particular realization of our estimator.

## {{< fa sign-hanging >}} Estimation
### Desirable Properties of Estimators

-   There are potentially _many_ estimators we can use to estimate a particular parameter.

-   As such, it is necessary to establish a notion of what makes a "good" estimator (or, equivalently, what makes one estimator "better" than another).

-   One notion is [**unbiasedness**]{.alert}: an estimator $\widehat{\theta}_n$ for $\theta$ is said to be _unbiased_ if $\E[\widehat{\theta}_n] = \theta$.
    -   "On average, the estimator gets it right."
    -   Mathematically: means the sampling distribution is centered at the right (true) value.
  

## {{< fa sign-hanging >}} Estimation
### An Analogy

-   Unbiasedness, however, is often not enough. To motivate why, let's take a look at an analogy.

-   An analogy is often drawn between estimation and hitting a bullseye.
    -   The bullseye is akin to our estimand, and estimates are represented by shots fired at the target. 
    -   The estimator is, therefore, akin to the marskperson.
    
-   An unbiased estimator is analogous to a marksperson for whom the average location of shots is the bullseye.

## {{< fa sign-hanging >}} Estimation
### Two Markspersons

-   Which of the following markspersons is "better"? 

:::: {.columns}

::: {.column width="50%"}
::: {.fragment style="text-align:center"}
![](Images/mark1.svg){width="50%"}

**Marksperson 1**
:::
:::


::: {.column width="50%"}
::: {.fragment style="text-align:center"}
![](Images/mark2.svg){width="50%"}

**Marksperson 2**
:::
:::

::::



## {{< fa sign-hanging >}} Estimation
### Two Markspersons

-   So, unbiasedness is not enough; we'd also like small _variance_.

-   To that end, we introduce the [**mean squared-error**]{.alert} (MSE) of an estimator:
$$ \mathrm{MSE}(\widehat{\theta}_n) := \E\left[(\widehat{\theta}_n - \theta)^2 \right] $$

::: {.fragment}
::: {.callout-important}
## **Bias-Variance Decomposition**
$$ \mathrm{MSE}(\widehat{\theta}_n) := \mathrm{Bias}^2(\widehat{\theta}_n) + \Var(\widehat{\theta}_n) $$

where $\mathrm{Bias}(\widehat{\theta}_n) := \E[\widehat{\theta}_n] - \theta$.
:::
:::
